//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_90a
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<45>;
	.reg .b32 	%r<193>;
	.reg .f32 	%f<143>;
	.reg .b64 	%rd<130>;
	.loc	1 35 0
$L__func_begin0:
	.loc	1 35 0

	ld.param.u32 	%r25, [matmul_kernel_param_8];
	ld.param.u32 	%r24, [matmul_kernel_param_7];
	ld.param.u32 	%r23, [matmul_kernel_param_6];
	ld.param.u32 	%r22, [matmul_kernel_param_5];
	ld.param.u64 	%rd34, [matmul_kernel_param_2];
	ld.param.u64 	%rd32, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 58 24
	// begin inline asm
	mov.u32 %r26, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r43, %r24, 15;
	.loc	2 44 28
	shr.s32 	%r44, %r43, 31;
	shr.u32 	%r45, %r44, 28;
	add.s32 	%r46, %r43, %r45;
	shr.s32 	%r47, %r46, 4;
	ld.param.s32 	%rd2, [matmul_kernel_param_3];
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r49, %r23, 63;
	.loc	2 44 28
	shr.s32 	%r50, %r49, 31;
	shr.u32 	%r51, %r50, 26;
	add.s32 	%r52, %r49, %r51;
	shr.s32 	%r53, %r52, 6;
$L__tmp3:
	.loc	1 61 38
	shl.b32 	%r54, %r47, 3;
	.loc	1 62 22
	div.s32 	%r56, %r26, %r54;
	.loc	1 63 29
	shl.b32 	%r57, %r56, 3;
	.loc	1 64 35
	sub.s32 	%r58, %r53, %r57;
	.loc	1 64 48
	min.s32 	%r59, %r58, 8;
	.loc	1 65 33
	rem.s32 	%r60, %r26, %r59;
	.loc	1 65 27
	add.s32 	%r61, %r57, %r60;
	mul.lo.s32 	%r62, %r56, %r54;
	sub.s32 	%r63, %r26, %r62;
	.loc	1 66 40
	div.s32 	%r64, %r63, %r59;
	.loc	1 67 29
	shl.b32 	%r65, %r61, 6;
	.loc	1 68 29
	shl.b32 	%r66, %r64, 4;
	.loc	1 76 8
	cvt.s64.s32 	%rd43, %r23;
	cvt.s64.s32 	%rd1, %r25;
	cvt.s64.s32 	%rd3, %r65;
	.loc	1 84 8
	cvt.s64.s32 	%rd5, %r66;
	.loc	1 88 43
	mov.u32 	%r1, %tid.x;
	bfe.u32 	%r3, %r1, 5, 2;
	bfe.u32 	%r4, %r1, 2, 3;
	shl.b32 	%r5, %r3, 3;
	or.b32  	%r6, %r5, %r4;
	bfe.u32 	%r7, %r1, 3, 2;
	shl.b32 	%r8, %r3, 2;
	or.b32  	%r67, %r8, %r7;
	or.b32  	%r68, %r67, 16;
	or.b32  	%r69, %r67, 32;
	or.b32  	%r70, %r67, 48;
	.loc	1 89 43
	and.b32  	%r9, %r1, 3;
	shl.b32 	%r10, %r9, 2;
	cvt.u64.u32 	%rd45, %r67;
	cvt.u64.u32 	%rd46, %r68;
	cvt.u64.u32 	%rd47, %r69;
	cvt.u64.u32 	%rd48, %r70;
	.loc	1 94 20
	or.b64  	%rd49, %rd3, %rd45;
	or.b64  	%rd50, %rd3, %rd46;
	or.b64  	%rd51, %rd3, %rd47;
	or.b64  	%rd52, %rd3, %rd48;
	mul.lo.s64 	%rd53, %rd49, %rd2;
	mul.lo.s64 	%rd54, %rd50, %rd2;
	mul.lo.s64 	%rd55, %rd51, %rd2;
	mul.lo.s64 	%rd56, %rd52, %rd2;
	shl.b32 	%r71, %r1, 2;
	and.b32  	%r72, %r71, 28;
	cvt.u64.u32 	%rd6, %r72;
	setp.gt.s64 	%p14, %rd49, -1;
	setp.gt.s64 	%p15, %rd50, -1;
	setp.gt.s64 	%p16, %rd51, -1;
	setp.gt.s64 	%p17, %rd52, -1;
	setp.lt.s64 	%p18, %rd49, %rd43;
	setp.lt.s64 	%p19, %rd50, %rd43;
	setp.lt.s64 	%p20, %rd51, %rd43;
	setp.lt.s64 	%p21, %rd52, %rd43;
	and.pred  	%p1, %p14, %p18;
	and.pred  	%p2, %p15, %p19;
	and.pred  	%p3, %p16, %p20;
	and.pred  	%p4, %p17, %p21;
	.loc	1 93 25
	setp.lt.s32 	%p24, %r25, 1;
	setp.gt.s32 	%p25, %r25, 0;
	.loc	1 94 20
	shl.b64 	%rd59, %rd53, 2;
	add.s64 	%rd60, %rd32, %rd59;
	mul.wide.u32 	%rd61, %r72, 4;
	add.s64 	%rd35, %rd60, %rd61;
	shl.b64 	%rd62, %rd54, 2;
	add.s64 	%rd63, %rd32, %rd62;
	add.s64 	%rd36, %rd63, %rd61;
	shl.b64 	%rd64, %rd55, 2;
	add.s64 	%rd65, %rd32, %rd64;
	add.s64 	%rd37, %rd65, %rd61;
	shl.b64 	%rd66, %rd56, 2;
	add.s64 	%rd67, %rd32, %rd66;
	add.s64 	%rd38, %rd67, %rd61;
	setp.lt.s32 	%p26, %r72, %r25;
	xor.b32  	%r73, %r67, %r1;
	shl.b32 	%r74, %r67, 7;
	shl.b32 	%r75, %r73, 4;
	and.b32  	%r76, %r75, 112;
	or.b32  	%r77, %r74, %r76;
	mov.u32 	%r78, global_smem;
	add.s32 	%r27, %r78, %r77;
	add.s32 	%r29, %r27, 2048;
	add.s32 	%r31, %r27, 4096;
	add.s32 	%r33, %r27, 6144;
	selp.b32 	%r79, 16, 0, %p1;
	selp.b32 	%r80, %r79, 0, %p26;
	selp.b32 	%r28, %r80, 0, %p25;
	mov.pred 	%p6, -1;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r27 + 0 ], [ %rd35 + 0 ], 0x10, %r28;
	// end inline asm
	selp.b32 	%r81, 16, 0, %p2;
	selp.b32 	%r82, %r81, 0, %p26;
	selp.b32 	%r30, %r82, 0, %p25;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r29 + 0 ], [ %rd36 + 0 ], 0x10, %r30;
	// end inline asm
	selp.b32 	%r83, 16, 0, %p3;
	selp.b32 	%r84, %r83, 0, %p26;
	selp.b32 	%r32, %r84, 0, %p25;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r31 + 0 ], [ %rd37 + 0 ], 0x10, %r32;
	// end inline asm
	selp.b32 	%r85, 16, 0, %p4;
	selp.b32 	%r86, %r85, 0, %p26;
	selp.b32 	%r34, %r86, 0, %p25;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r33 + 0 ], [ %rd38 + 0 ], 0x10, %r34;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	setp.gt.s32 	%p27, %r25, 32;
	.loc	1 94 20
	or.b64  	%rd68, %rd6, 32;
	add.s64 	%rd39, %rd35, 128;
	add.s64 	%rd40, %rd36, 128;
	add.s64 	%rd41, %rd37, 128;
	add.s64 	%rd42, %rd38, 128;
	setp.lt.s64 	%p28, %rd68, %rd1;
	bar.sync 	0;
	add.s32 	%r35, %r27, 8192;
	add.s32 	%r37, %r27, 10240;
	add.s32 	%r39, %r27, 12288;
	add.s32 	%r41, %r27, 14336;
	selp.b32 	%r87, %r79, 0, %p28;
	selp.b32 	%r36, %r87, 0, %p27;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r35 + 0 ], [ %rd39 + 0 ], 0x10, %r36;
	// end inline asm
	selp.b32 	%r88, %r81, 0, %p28;
	selp.b32 	%r38, %r88, 0, %p27;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r37 + 0 ], [ %rd40 + 0 ], 0x10, %r38;
	// end inline asm
	selp.b32 	%r89, %r83, 0, %p28;
	selp.b32 	%r40, %r89, 0, %p27;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r39 + 0 ], [ %rd41 + 0 ], 0x10, %r40;
	// end inline asm
	selp.b32 	%r90, %r85, 0, %p28;
	selp.b32 	%r42, %r90, 0, %p27;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r41 + 0 ], [ %rd42 + 0 ], 0x10, %r42;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	mov.f32 	%f127, 0f00000000;
	mov.f32 	%f128, %f127;
	mov.f32 	%f129, %f127;
	mov.f32 	%f130, %f127;
	mov.f32 	%f131, %f127;
	mov.f32 	%f132, %f127;
	mov.f32 	%f133, %f127;
	mov.f32 	%f134, %f127;
	.loc	1 93 25
	@%p24 bra 	$L__BB0_3;
	.loc	1 0 25
	ld.param.u64 	%rd33, [matmul_kernel_param_1];
	ld.param.s32 	%rd4, [matmul_kernel_param_4];
	cvt.s64.s32 	%rd44, %r24;
	shr.u32 	%r2, %r1, 5;
	cvt.u64.u32 	%rd57, %r10;
	or.b64  	%rd58, %rd5, %rd57;
	setp.gt.s64 	%p22, %rd58, -1;
	setp.lt.s64 	%p23, %rd58, %rd44;
	and.pred  	%p5, %p22, %p23;
	add.s32 	%r12, %r25, -64;
	or.b32  	%r93, %r10, 1;
	or.b32  	%r94, %r10, 2;
	or.b32  	%r95, %r10, 3;
	and.b32  	%r96, %r10, 4;
	shl.b32 	%r97, %r9, 7;
	shr.u32 	%r98, %r6, 2;
	xor.b32  	%r99, %r98, %r96;
	shl.b32 	%r100, %r99, 2;
	and.b32  	%r101, %r4, 3;
	or.b32  	%r102, %r97, %r100;
	or.b32  	%r103, %r102, %r101;
	shl.b32 	%r104, %r103, 2;
	add.s32 	%r106, %r78, 24576;
	add.s32 	%r13, %r106, %r104;
	and.b32  	%r107, %r93, 5;
	shl.b32 	%r108, %r93, 5;
	xor.b32  	%r109, %r98, %r107;
	shl.b32 	%r110, %r109, 2;
	or.b32  	%r111, %r108, %r110;
	or.b32  	%r112, %r111, %r101;
	shl.b32 	%r113, %r112, 2;
	add.s32 	%r14, %r106, %r113;
	and.b32  	%r114, %r94, 6;
	shl.b32 	%r115, %r94, 5;
	xor.b32  	%r116, %r98, %r114;
	shl.b32 	%r117, %r116, 2;
	or.b32  	%r118, %r115, %r117;
	or.b32  	%r119, %r118, %r101;
	shl.b32 	%r120, %r119, 2;
	add.s32 	%r15, %r106, %r120;
	and.b32  	%r121, %r95, 7;
	shl.b32 	%r122, %r95, 5;
	xor.b32  	%r123, %r98, %r121;
	shl.b32 	%r124, %r123, 2;
	or.b32  	%r125, %r122, %r124;
	or.b32  	%r126, %r125, %r101;
	shl.b32 	%r127, %r126, 2;
	add.s32 	%r16, %r106, %r127;
	and.b32  	%r17, %r2, 134217724;
	shr.u32 	%r128, %r106, 4;
	cvt.u64.u32 	%rd70, %r128;
	and.b64  	%rd71, %rd70, 16383;
	or.b64  	%rd100, %rd71, 4611686293313683456;
	add.s64 	%rd102, %rd71, 4611686293313683458;
	add.s64 	%rd104, %rd71, 4611686293313683460;
	add.s64 	%rd106, %rd71, 4611686293313683462;
	.loc	1 93 25
	add.s32 	%r129, %r4, %r5;
	cvt.u64.u32 	%rd11, %r129;
	mul.lo.s64 	%rd72, %rd4, %rd11;
	shl.b64 	%rd73, %rd72, 2;
	mul.wide.u32 	%rd74, %r9, 16;
	add.s64 	%rd75, %rd73, %rd74;
	shl.b64 	%rd76, %rd5, 2;
	add.s64 	%rd77, %rd75, %rd76;
	add.s64 	%rd128, %rd33, %rd77;
	shl.b64 	%rd13, %rd4, 7;
	and.b32  	%r131, %r1, 7;
	mul.wide.u32 	%rd14, %r131, 16;
	add.s32 	%r132, %r7, %r8;
	add.s32 	%r133, %r132, 48;
	cvt.u64.u32 	%rd78, %r133;
	add.s64 	%rd79, %rd3, %rd78;
	mul.lo.s64 	%rd80, %rd79, %rd2;
	shl.b64 	%rd81, %rd80, 2;
	add.s64 	%rd82, %rd81, %rd32;
	add.s64 	%rd127, %rd82, 256;
	or.b32  	%r134, %r132, 32;
	cvt.u64.u32 	%rd83, %r134;
	add.s64 	%rd84, %rd3, %rd83;
	mul.lo.s64 	%rd85, %rd84, %rd2;
	shl.b64 	%rd86, %rd85, 2;
	add.s64 	%rd87, %rd86, %rd32;
	add.s64 	%rd126, %rd87, 256;
	add.s32 	%r135, %r132, 16;
	cvt.u64.u32 	%rd88, %r135;
	add.s64 	%rd89, %rd3, %rd88;
	mul.lo.s64 	%rd90, %rd89, %rd2;
	shl.b64 	%rd91, %rd90, 2;
	add.s64 	%rd92, %rd91, %rd32;
	add.s64 	%rd125, %rd92, 256;
	cvt.u64.u32 	%rd93, %r132;
	add.s64 	%rd94, %rd3, %rd93;
	mul.lo.s64 	%rd95, %rd94, %rd2;
	shl.b64 	%rd96, %rd95, 2;
	add.s64 	%rd97, %rd96, %rd32;
	add.s64 	%rd124, %rd97, 256;
	or.b64  	%rd19, %rd6, 64;
	mov.f32 	%f120, 0f00000000;
	mov.u64 	%rd129, 0;
	mov.b32 	%r192, 1;
	mov.b32 	%r191, -1;
	mov.f32 	%f119, 0f00000001;
	mov.f32 	%f118, 0f00000020;
	mov.f32 	%f127, %f120;
	mov.f32 	%f128, %f120;
	mov.f32 	%f129, %f120;
	mov.f32 	%f130, %f120;
	mov.f32 	%f131, %f120;
	mov.f32 	%f132, %f120;
	mov.f32 	%f133, %f120;
	mov.f32 	%f134, %f120;
$L__BB0_2:
	.loc	1 0 25
	cvt.u32.u64 	%r148, %rd129;
	.loc	1 93 25
	setp.lt.s32 	%p34, %r148, %r12;
	add.s32 	%r149, %r191, 1;
	setp.lt.s32 	%p35, %r149, 3;
	selp.b32 	%r191, %r149, 0, %p35;
	.loc	1 94 20
	// begin inline asm
	cp.async.wait_group 0x1;
	// end inline asm
	bar.sync 	0;
	shl.b32 	%r150, %r191, 13;
	add.s32 	%r152, %r78, %r150;
	.loc	1 95 20
	add.s64 	%rd111, %rd11, %rd129;
	setp.lt.u64 	%p36, %rd111, %rd1;
	and.pred  	%p29, %p5, %p36;
	// begin inline asm
	mov.u32 %r136, 0x0;
	mov.u32 %r137, 0x0;
	mov.u32 %r138, 0x0;
	mov.u32 %r139, 0x0;
	@%p29 ld.global.v4.b32 { %r136, %r137, %r138, %r139 }, [ %rd128 + 0 ];
	// end inline asm
	st.shared.u32 	[%r13], %r136;
	st.shared.u32 	[%r14], %r137;
	st.shared.u32 	[%r15], %r138;
	st.shared.u32 	[%r16], %r139;
	.loc	1 96 23
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	bar.sync 	0;
	shfl.sync.idx.b32	%r153, %r17, 0, 31, -1;
	// begin inline asm
	wgmma.fence.sync.aligned;
	// end inline asm
	shl.b32 	%r154, %r153, 7;
	and.b32  	%r155, %r154, 384;
	cvt.u64.u32 	%rd112, %r155;
	shr.u32 	%r156, %r152, 4;
	cvt.u64.u32 	%rd113, %r156;
	and.b64  	%rd114, %rd113, 16383;
	add.s64 	%rd115, %rd114, %rd112;
	or.b64  	%rd99, %rd115, 4611686293338849280;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32 {%f127,%f128,%f129,%f130,%f131,%f132,%f133,%f134}, %rd99, %rd100, 1, 1, 1;
	// end inline asm
	add.s64 	%rd101, %rd115, 4611686293338849282;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32 {%f127,%f128,%f129,%f130,%f131,%f132,%f133,%f134}, %rd101, %rd102, 1, 1, 1;
	// end inline asm
	add.s64 	%rd103, %rd115, 4611686293338849284;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32 {%f127,%f128,%f129,%f130,%f131,%f132,%f133,%f134}, %rd103, %rd104, 1, 1, 1;
	// end inline asm
	add.s64 	%rd105, %rd115, 4611686293338849286;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32 {%f127,%f128,%f129,%f130,%f131,%f132,%f133,%f134}, %rd105, %rd106, 1, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;
	// end inline asm
	mov.b32 	%f99, %r152;
	mov.b32 	%f104, %r106;
	mov.f32 	%f100, %f118;
	mov.f32 	%f102, %f120;
	mov.f32 	%f103, %f120;
	mov.f32 	%f107, %f120;
	mov.f32 	%f106, %f118;
	mov.f32 	%f101, %f119;
	mov.f32 	%f108, %f120;
	mov.f32 	%f105, %f119;
	// begin inline asm
	// wait for regs: %f127,%f128,%f129,%f130,%f131,%f132,%f133,%f134,%f99,%f100,%f101,%f102,%f103,%f104,%f105,%f106,%f107,%f108
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	.loc	1 98 44
	add.s64 	%rd26, %rd129, 32;
	.loc	1 93 25
	add.s32 	%r158, %r192, 1;
	setp.lt.s32 	%p37, %r158, 3;
	selp.b32 	%r192, %r158, 0, %p37;
	.loc	1 94 20
	add.s64 	%rd116, %rd19, %rd129;
	add.s64 	%rd107, %rd124, %rd14;
	add.s64 	%rd108, %rd125, %rd14;
	add.s64 	%rd109, %rd126, %rd14;
	add.s64 	%rd110, %rd127, %rd14;
	setp.lt.u64 	%p38, %rd116, %rd1;
	bar.sync 	0;
	shl.b32 	%r159, %r192, 13;
	add.s32 	%r140, %r27, %r159;
	add.s32 	%r142, %r140, 2048;
	add.s32 	%r144, %r140, 4096;
	add.s32 	%r146, %r140, 6144;
	selp.b32 	%r160, 16, 0, %p38;
	selp.b32 	%r161, %r160, 0, %p1;
	selp.b32 	%r141, %r161, 0, %p34;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r140 + 0 ], [ %rd107 + 0 ], 0x10, %r141;
	// end inline asm
	selp.b32 	%r162, %r160, 0, %p2;
	selp.b32 	%r143, %r162, 0, %p34;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r142 + 0 ], [ %rd108 + 0 ], 0x10, %r143;
	// end inline asm
	selp.b32 	%r163, %r160, 0, %p3;
	selp.b32 	%r145, %r163, 0, %p34;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r144 + 0 ], [ %rd109 + 0 ], 0x10, %r145;
	// end inline asm
	selp.b32 	%r164, %r160, 0, %p4;
	selp.b32 	%r147, %r164, 0, %p34;
	// begin inline asm
	@%p6 cp.async.cg.shared.global [ %r146 + 0 ], [ %rd110 + 0 ], 0x10, %r147;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	add.s64 	%rd128, %rd128, %rd13;
	add.s64 	%rd127, %rd127, 128;
	add.s64 	%rd126, %rd126, 128;
	add.s64 	%rd125, %rd125, 128;
	add.s64 	%rd124, %rd124, 128;
	cvt.u32.u64 	%r165, %rd26;
	setp.lt.s32 	%p39, %r165, %r25;
	mov.u64 	%rd129, %rd26;
	@%p39 bra 	$L__BB0_2;
$L__BB0_3:
	.loc	1 0 25
	cvt.u32.u64 	%r174, %rd3;
	cvt.u32.u64 	%r175, %rd5;
	.loc	1 88 30
	or.b32  	%r176, %r174, %r6;
	or.b32  	%r177, %r176, 32;
	.loc	1 91 21
	setp.lt.s32 	%p42, %r177, %r23;
	.loc	1 89 30
	or.b32  	%r178, %r175, %r10;
	.loc	1 91 45
	setp.lt.s32 	%p43, %r178, %r24;
	.loc	1 91 35
	and.pred  	%p41, %p42, %p43;
	.loc	1 91 21
	setp.lt.s32 	%p44, %r176, %r23;
	.loc	1 91 35
	and.pred  	%p40, %p44, %p43;
	.loc	1 90 39
	mul.lo.s32 	%r179, %r177, %r22;
	.loc	1 90 21
	mul.wide.s32 	%rd119, %r179, 4;
	add.s64 	%rd120, %rd34, %rd119;
	.loc	1 90 51
	mul.wide.s32 	%rd121, %r178, 4;
	add.s64 	%rd118, %rd120, %rd121;
	.loc	1 90 39
	mul.lo.s32 	%r180, %r176, %r22;
	.loc	1 90 21
	mul.wide.s32 	%rd122, %r180, 4;
	add.s64 	%rd123, %rd34, %rd122;
	.loc	1 90 51
	add.s64 	%rd117, %rd123, %rd121;
	.loc	1 93 25
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 102 21
	shl.b32 	%r181, %r9, 1;
	shl.b32 	%r182, %r3, 4;
	or.b32  	%r183, %r182, %r4;
	mad.lo.s32 	%r184, %r183, 20, %r181;
	shl.b32 	%r185, %r184, 2;
	add.s32 	%r187, %r78, %r185;
	st.shared.v2.f32 	[%r187], {%f127, %f128};
	st.shared.v2.f32 	[%r187+640], {%f129, %f130};
	st.shared.v2.f32 	[%r187+32], {%f131, %f132};
	st.shared.v2.f32 	[%r187+672], {%f133, %f134};
	bar.sync 	0;
	mad.lo.s32 	%r188, %r6, 20, %r10;
	shl.b32 	%r189, %r188, 2;
	add.s32 	%r190, %r78, %r189;
	ld.shared.v4.u32 	{%r170, %r171, %r172, %r173}, [%r190+2560];
	ld.shared.v4.u32 	{%r166, %r167, %r168, %r169}, [%r190];
	// begin inline asm
	@%p40 st.global.v4.b32 [ %rd117 + 0 ], { %r166, %r167, %r168, %r169 };
	// end inline asm
	// begin inline asm
	@%p41 st.global.v4.b32 [ %rd118 + 0 ], { %r170, %r171, %r172, %r173 };
	// end inline asm
	.loc	1 102 4
	ret;
$L__tmp4:
$L__func_end0:

}
	.file	1 "/opt/dlami/nvme/csullivan/triton/python/test/unit/hopper/test_gemm_f8.py"
	.file	2 "/opt/dlami/nvme/csullivan/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 197
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 116
.b8 101
.b8 115
.b8 116
.b8 95
.b8 103
.b8 101
.b8 109
.b8 109
.b8 95
.b8 102
.b8 56
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 111
.b8 112
.b8 116
.b8 47
.b8 100
.b8 108
.b8 97
.b8 109
.b8 105
.b8 47
.b8 110
.b8 118
.b8 109
.b8 101
.b8 47
.b8 99
.b8 115
.b8 117
.b8 108
.b8 108
.b8 105
.b8 118
.b8 97
.b8 110
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 112
.b8 121
.b8 116
.b8 104
.b8 111
.b8 110
.b8 47
.b8 116
.b8 101
.b8 115
.b8 116
.b8 47
.b8 117
.b8 110
.b8 105
.b8 116
.b8 47
.b8 104
.b8 111
.b8 112
.b8 112
.b8 101
.b8 114
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 114
.b8 4
.b32 114
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 59
.b8 27
.b8 4
.b32 114
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 60
.b8 27
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

