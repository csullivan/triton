//
// Generated by LLVM NVPTX Back-End
//

.version 8.3
.target sm_90a
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<26>;
	.reg .b32 	%r<149>;
	.reg .f32 	%f<109>;
	.reg .b64 	%rd<62>;
	.loc	1 35 0
$L__func_begin0:
	.loc	1 35 0

	ld.param.u32 	%r34, [matmul_kernel_param_8];
	ld.param.u32 	%r33, [matmul_kernel_param_7];
	ld.param.u32 	%r32, [matmul_kernel_param_6];
	ld.param.u32 	%r31, [matmul_kernel_param_5];
	ld.param.u64 	%rd20, [matmul_kernel_param_2];
	ld.param.u64 	%rd18, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 58 24
	// begin inline asm
	mov.u32 %r35, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r40, %r33, 15;
	.loc	2 44 28
	shr.s32 	%r41, %r40, 31;
	shr.u32 	%r42, %r41, 28;
	add.s32 	%r43, %r40, %r42;
	shr.s32 	%r44, %r43, 4;
	ld.param.s32 	%rd2, [matmul_kernel_param_3];
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r46, %r32, 63;
	.loc	2 44 28
	shr.s32 	%r47, %r46, 31;
	shr.u32 	%r48, %r47, 26;
	add.s32 	%r49, %r46, %r48;
	shr.s32 	%r50, %r49, 6;
$L__tmp3:
	.loc	1 61 38
	shl.b32 	%r51, %r44, 3;
	.loc	1 62 22
	div.s32 	%r53, %r35, %r51;
	.loc	1 63 29
	shl.b32 	%r54, %r53, 3;
	.loc	1 64 35
	sub.s32 	%r55, %r50, %r54;
	.loc	1 64 48
	min.s32 	%r56, %r55, 8;
	.loc	1 65 33
	rem.s32 	%r57, %r35, %r56;
	.loc	1 65 27
	add.s32 	%r58, %r54, %r57;
	mul.lo.s32 	%r59, %r53, %r51;
	sub.s32 	%r60, %r35, %r59;
	.loc	1 66 40
	div.s32 	%r61, %r60, %r56;
	.loc	1 67 29
	shl.b32 	%r62, %r58, 6;
	.loc	1 68 29
	shl.b32 	%r63, %r61, 4;
	.loc	1 76 8
	cvt.s64.s32 	%rd23, %r32;
	cvt.s64.s32 	%rd1, %r34;
	cvt.s64.s32 	%rd3, %r62;
	.loc	1 84 8
	cvt.s64.s32 	%rd5, %r63;
	.loc	1 88 43
	mov.u32 	%r1, %tid.x;
	bfe.u32 	%r3, %r1, 5, 2;
	bfe.u32 	%r4, %r1, 2, 3;
	shl.b32 	%r5, %r3, 3;
	or.b32  	%r6, %r5, %r4;
	bfe.u32 	%r7, %r1, 1, 4;
	shl.b32 	%r8, %r3, 4;
	or.b32  	%r64, %r8, %r7;
	.loc	1 89 43
	and.b32  	%r9, %r1, 3;
	shl.b32 	%r65, %r9, 2;
	cvt.u64.u32 	%rd25, %r64;
	.loc	1 94 20
	or.b64  	%rd26, %rd3, %rd25;
	mul.lo.s64 	%rd27, %rd26, %rd2;
	shl.b32 	%r66, %r1, 4;
	and.b32  	%r67, %r66, 16;
	cvt.u64.u32 	%rd6, %r67;
	setp.gt.s64 	%p5, %rd26, -1;
	setp.lt.s64 	%p6, %rd26, %rd23;
	and.pred  	%p1, %p5, %p6;
	cvt.u64.u32 	%rd7, %r65;
	.loc	1 95 20
	or.b64  	%rd28, %rd5, %rd7;
	.loc	1 93 25
	setp.gt.s32 	%p9, %r34, 0;
	.loc	1 94 20
	add.s64 	%rd29, %rd18, %rd27;
	add.s64 	%rd21, %rd29, %rd6;
	setp.lt.s32 	%p10, %r67, %r34;
	shl.b32 	%r68, %r64, 5;
	shl.b32 	%r10, %r1, 1;
	xor.b32  	%r69, %r66, %r10;
	and.b32  	%r70, %r69, 16;
	or.b32  	%r71, %r68, %r70;
	mov.u32 	%r72, global_smem;
	add.s32 	%r36, %r72, %r71;
	selp.b32 	%r73, 16, 0, %p1;
	selp.b32 	%r74, %r73, 0, %p9;
	selp.b32 	%r37, %r74, 0, %p10;
	mov.pred 	%p4, -1;
	// begin inline asm
	@%p4 cp.async.cg.shared.global [ %r36 + 0 ], [ %rd21 + 0 ], 0x10, %r37;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	setp.gt.s32 	%p11, %r34, 32;
	.loc	1 94 20
	or.b64  	%rd30, %rd6, 32;
	add.s64 	%rd22, %rd21, 32;
	setp.lt.s64 	%p12, %rd30, %rd1;
	bar.sync 	0;
	add.s32 	%r38, %r36, 2048;
	selp.b32 	%r75, %r73, 0, %p12;
	selp.b32 	%r39, %r75, 0, %p11;
	// begin inline asm
	@%p4 cp.async.cg.shared.global [ %r38 + 0 ], [ %rd22 + 0 ], 0x10, %r39;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	@%p9 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	.loc	1 0 25
	ld.param.u64 	%rd19, [matmul_kernel_param_1];
	ld.param.s32 	%rd4, [matmul_kernel_param_4];
	cvt.s64.s32 	%rd24, %r33;
	shr.u32 	%r2, %r1, 5;
	setp.gt.s64 	%p7, %rd28, -1;
	setp.lt.s64 	%p8, %rd28, %rd24;
	and.pred  	%p2, %p7, %p8;
	add.s32 	%r14, %r34, -64;
	and.b32  	%r80, %r1, 1;
	shl.b32 	%r81, %r9, 7;
	shr.u32 	%r82, %r3, 1;
	xor.b32  	%r83, %r82, %r80;
	shl.b32 	%r84, %r83, 4;
	and.b32  	%r85, %r6, 15;
	or.b32  	%r86, %r84, %r81;
	or.b32  	%r87, %r86, %r85;
	add.s32 	%r89, %r72, 6144;
	add.s32 	%r15, %r89, %r87;
	add.s32 	%r16, %r15, 32;
	add.s32 	%r17, %r15, 64;
	add.s32 	%r18, %r15, 96;
	and.b32  	%r19, %r2, 134217724;
	shr.u32 	%r90, %r89, 4;
	cvt.u64.u32 	%rd32, %r90;
	and.b64  	%rd33, %rd32, 16383;
	or.b64  	%rd44, %rd33, -4611685949705814016;
	shl.b32 	%r91, %r9, 1;
	or.b32  	%r92, %r8, %r4;
	mad.lo.s32 	%r93, %r92, 18, %r91;
	shl.b32 	%r94, %r93, 2;
	add.s32 	%r20, %r89, %r94;
	add.s32 	%r21, %r20, 32;
	shr.u32 	%r95, %r1, 2;
	and.b32  	%r96, %r95, 6;
	or.b32  	%r148, %r5, %r96;
	and.b32  	%r147, %r10, 14;
	mad.lo.s32 	%r97, %r148, 18, %r147;
	shl.b32 	%r98, %r97, 2;
	add.s32 	%r24, %r89, %r98;
	.loc	1 93 25
	add.s32 	%r99, %r4, %r5;
	cvt.u64.u32 	%rd9, %r99;
	mul.lo.s64 	%rd34, %rd4, %rd9;
	add.s64 	%rd35, %rd34, %rd7;
	add.s64 	%rd36, %rd35, %rd5;
	add.s64 	%rd60, %rd19, %rd36;
	shl.b64 	%rd11, %rd4, 5;
	add.s32 	%r100, %r7, %r8;
	cvt.u64.u32 	%rd37, %r100;
	add.s64 	%rd38, %rd3, %rd37;
	mul.lo.s64 	%rd39, %rd38, %rd2;
	add.s64 	%rd40, %rd39, %rd6;
	add.s64 	%rd41, %rd40, %rd18;
	add.s64 	%rd12, %rd41, 64;
	or.b64  	%rd13, %rd6, 64;
	mov.f32 	%f39, 0f00000000;
	mov.u64 	%rd61, 0;
	mov.b32 	%r146, 1;
	mov.b32 	%r145, -1;
	mov.f32 	%f101, %f39;
	mov.f32 	%f102, %f39;
	mov.f32 	%f103, %f39;
	mov.f32 	%f104, %f39;
	mov.f32 	%f105, %f39;
	mov.f32 	%f106, %f39;
	mov.f32 	%f107, %f39;
	mov.f32 	%f108, %f39;
$L__BB0_3:
	.loc	1 0 25
	cvt.u32.u64 	%r104, %rd61;
	.loc	1 93 25
	setp.lt.s32 	%p15, %r104, %r14;
	add.s32 	%r105, %r145, 1;
	setp.lt.s32 	%p16, %r105, 3;
	selp.b32 	%r145, %r105, 0, %p16;
	.loc	1 94 20
	// begin inline asm
	cp.async.wait_group 0x1;
	// end inline asm
	bar.sync 	0;
	shl.b32 	%r106, %r145, 11;
	add.s32 	%r108, %r72, %r106;
	.loc	1 95 20
	add.s64 	%rd46, %rd9, %rd61;
	setp.lt.u64 	%p17, %rd46, %rd1;
	and.pred  	%p13, %p2, %p17;
	// begin inline asm
	mov.u32 %r101, 0x0;
	@%p13 ld.global.b32 { %r101 }, [ %rd60 + 0 ];
	// end inline asm
	st.shared.u8 	[%r15], %r101;
	bfe.u32 	%r110, %r101, 8, 8;
	st.shared.u8 	[%r16], %r110;
	bfe.u32 	%r111, %r101, 16, 8;
	st.shared.u8 	[%r17], %r111;
	bfe.u32 	%r112, %r101, 24, 8;
	st.shared.u8 	[%r18], %r112;
	.loc	1 96 23
	// begin inline asm
	fence.proxy.async.shared::cta;
	// end inline asm
	bar.sync 	0;
	shfl.sync.idx.b32	%r113, %r19, 0, 31, -1;
	// begin inline asm
	wgmma.fence.sync.aligned;
	// end inline asm
	shl.b32 	%r114, %r113, 5;
	and.b32  	%r115, %r114, 96;
	cvt.u64.u32 	%rd47, %r115;
	shr.u32 	%r116, %r108, 4;
	cvt.u64.u32 	%rd48, %r116;
	and.b64  	%rd49, %rd48, 16383;
	add.s64 	%rd50, %rd47, %rd49;
	or.b64  	%rd43, %rd50, -4611685949699522560;
	// begin inline asm
	wgmma.mma_async.sync.aligned.m64n16k32.f32.e5m2.e5m2 {%f49,%f50,%f51,%f52,%f53,%f54,%f55,%f56}, %rd43, %rd44, 0, 1, 1;
	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;
	// end inline asm
	mov.b32 	%f57, %r108;
	mov.b32 	%f62, %r89;
	mov.f32 	%f59, 0f00000001;
	mov.f32 	%f64, 0f00000020;
	mov.f32 	%f66, %f39;
	mov.f32 	%f63, %f59;
	mov.f32 	%f58, %f64;
	mov.f32 	%f60, %f39;
	mov.f32 	%f61, %f39;
	mov.f32 	%f65, %f39;
	// begin inline asm
	// wait for regs: %f49,%f50,%f51,%f52,%f53,%f54,%f55,%f56,%f57,%f58,%f59,%f60,%f61,%f62,%f63,%f64,%f65,%f66
	wgmma.wait_group.sync.aligned 0;
	// end inline asm
	bar.sync 	0;
	st.shared.v2.f32 	[%r20], {%f49, %f50};
	st.shared.v2.f32 	[%r20+576], {%f51, %f52};
	st.shared.v2.f32 	[%r20+32], {%f53, %f54};
	st.shared.v2.f32 	[%r21+576], {%f55, %f56};
	bar.sync 	0;
	ld.shared.v2.f32 	{%f85, %f86}, [%r24];
	ld.shared.v2.f32 	{%f87, %f88}, [%r24+72];
	ld.shared.v2.f32 	{%f89, %f90}, [%r24+2304];
	ld.shared.v2.f32 	{%f91, %f92}, [%r24+2376];
	.loc	1 96 13
	add.f32 	%f108, %f108, %f92;
	add.f32 	%f107, %f107, %f91;
	add.f32 	%f106, %f106, %f90;
	add.f32 	%f105, %f105, %f89;
	add.f32 	%f104, %f104, %f88;
	add.f32 	%f103, %f103, %f87;
	add.f32 	%f102, %f102, %f86;
	add.f32 	%f101, %f101, %f85;
	.loc	1 98 44
	add.s64 	%rd16, %rd61, 32;
	.loc	1 93 25
	add.s32 	%r118, %r146, 1;
	setp.lt.s32 	%p18, %r118, 3;
	selp.b32 	%r146, %r118, 0, %p18;
	.loc	1 94 20
	add.s64 	%rd51, %rd13, %rd61;
	add.s64 	%rd45, %rd12, %rd61;
	setp.lt.u64 	%p19, %rd51, %rd1;
	shl.b32 	%r119, %r146, 11;
	add.s32 	%r102, %r36, %r119;
	selp.b32 	%r120, 16, 0, %p19;
	selp.b32 	%r121, %r120, 0, %p15;
	selp.b32 	%r103, %r121, 0, %p1;
	// begin inline asm
	@%p4 cp.async.cg.shared.global [ %r102 + 0 ], [ %rd45 + 0 ], 0x10, %r103;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	add.s64 	%rd60, %rd60, %rd11;
	cvt.u32.u64 	%r122, %rd16;
	setp.lt.s32 	%p20, %r122, %r34;
	mov.u64 	%rd61, %rd16;
	@%p20 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_4;
$L__BB0_1:
	.loc	1 102 21
	shr.u32 	%r76, %r1, 2;
	and.b32  	%r77, %r76, 6;
	or.b32  	%r148, %r5, %r77;
	and.b32  	%r147, %r10, 14;
	mov.f32 	%f101, 0f00000000;
	mov.f32 	%f102, %f101;
	mov.f32 	%f103, %f101;
	mov.f32 	%f104, %f101;
	mov.f32 	%f105, %f101;
	mov.f32 	%f106, %f101;
	mov.f32 	%f107, %f101;
	mov.f32 	%f108, %f101;
$L__BB0_4:
	.loc	1 0 21
	cvt.u32.u64 	%r131, %rd7;
	cvt.u32.u64 	%r132, %rd3;
	.loc	1 88 43
	or.b32  	%r133, %r132, %r6;
	.loc	1 88 30
	or.b32  	%r134, %r133, 32;
	.loc	1 91 21
	setp.lt.s32 	%p23, %r134, %r32;
	.loc	1 89 30
	cvt.u32.u64 	%r135, %rd28;
	.loc	1 91 45
	setp.lt.s32 	%p24, %r135, %r33;
	.loc	1 91 35
	and.pred  	%p22, %p23, %p24;
	.loc	1 91 21
	setp.lt.s32 	%p25, %r133, %r32;
	.loc	1 91 35
	and.pred  	%p21, %p25, %p24;
	.loc	1 90 39
	mul.lo.s32 	%r136, %r134, %r31;
	.loc	1 90 21
	mul.wide.s32 	%rd55, %r136, 4;
	add.s64 	%rd56, %rd20, %rd55;
	.loc	1 90 51
	mul.wide.s32 	%rd57, %r135, 4;
	add.s64 	%rd53, %rd56, %rd57;
	.loc	1 90 39
	mul.lo.s32 	%r137, %r133, %r31;
	.loc	1 90 21
	mul.wide.s32 	%rd58, %r137, 4;
	add.s64 	%rd59, %rd20, %rd58;
	.loc	1 90 51
	add.s64 	%rd52, %rd59, %rd57;
	.loc	1 93 25
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 102 21
	mad.lo.s32 	%r138, %r148, 20, %r147;
	shl.b32 	%r139, %r138, 2;
	add.s32 	%r141, %r72, %r139;
	st.shared.v2.f32 	[%r141], {%f101, %f102};
	st.shared.v2.f32 	[%r141+80], {%f103, %f104};
	bar.sync 	0;
	mad.lo.s32 	%r142, %r6, 20, %r131;
	shl.b32 	%r143, %r142, 2;
	add.s32 	%r144, %r72, %r143;
	ld.shared.v4.u32 	{%r123, %r124, %r125, %r126}, [%r144];
	bar.sync 	0;
	st.shared.v2.f32 	[%r141], {%f105, %f106};
	st.shared.v2.f32 	[%r141+80], {%f107, %f108};
	bar.sync 	0;
	ld.shared.v4.u32 	{%r127, %r128, %r129, %r130}, [%r144];
	// begin inline asm
	@%p21 st.global.v4.b32 [ %rd52 + 0 ], { %r123, %r124, %r125, %r126 };
	// end inline asm
	// begin inline asm
	@%p22 st.global.v4.b32 [ %rd53 + 0 ], { %r127, %r128, %r129, %r130 };
	// end inline asm
	.loc	1 102 4
	ret;
$L__tmp4:
$L__func_end0:

}
	.file	1 "/opt/dlami/nvme/csullivan/triton/python/test/unit/hopper/test_gemm_f8.py"
	.file	2 "/opt/dlami/nvme/csullivan/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 197
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 116
.b8 101
.b8 115
.b8 116
.b8 95
.b8 103
.b8 101
.b8 109
.b8 109
.b8 95
.b8 102
.b8 56
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 111
.b8 112
.b8 116
.b8 47
.b8 100
.b8 108
.b8 97
.b8 109
.b8 105
.b8 47
.b8 110
.b8 118
.b8 109
.b8 101
.b8 47
.b8 99
.b8 115
.b8 117
.b8 108
.b8 108
.b8 105
.b8 118
.b8 97
.b8 110
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 112
.b8 121
.b8 116
.b8 104
.b8 111
.b8 110
.b8 47
.b8 116
.b8 101
.b8 115
.b8 116
.b8 47
.b8 117
.b8 110
.b8 105
.b8 116
.b8 47
.b8 104
.b8 111
.b8 112
.b8 112
.b8 101
.b8 114
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 114
.b8 4
.b32 114
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 59
.b8 27
.b8 4
.b32 114
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 60
.b8 27
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

