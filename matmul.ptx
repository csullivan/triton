//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_90a
.address_size 64

	// .globl	matmul_kernel
.extern .shared .align 16 .b8 global_smem[];

.visible .entry matmul_kernel(
	.param .u64 matmul_kernel_param_0,
	.param .u64 matmul_kernel_param_1,
	.param .u64 matmul_kernel_param_2,
	.param .u32 matmul_kernel_param_3,
	.param .u32 matmul_kernel_param_4,
	.param .u32 matmul_kernel_param_5,
	.param .u32 matmul_kernel_param_6,
	.param .u32 matmul_kernel_param_7,
	.param .u32 matmul_kernel_param_8
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<183>;
	.reg .f32 	%f<23>;
	.reg .b64 	%rd<60>;
	.loc	1 35 0
$L__func_begin0:
	.loc	1 35 0

	ld.param.u32 	%r34, [matmul_kernel_param_8];
	ld.param.u32 	%r33, [matmul_kernel_param_7];
	ld.param.u32 	%r32, [matmul_kernel_param_6];
	ld.param.u32 	%r31, [matmul_kernel_param_5];
	ld.param.u64 	%rd22, [matmul_kernel_param_2];
	ld.param.u64 	%rd21, [matmul_kernel_param_1];
	ld.param.u64 	%rd20, [matmul_kernel_param_0];
$L__tmp0:
	.loc	1 58 24
	// begin inline asm
	mov.u32 %r35, %ctaid.x;
	// end inline asm
$L__tmp1:
	.loc	2 44 22
	add.s32 	%r44, %r33, 15;
	.loc	2 44 28
	shr.s32 	%r45, %r44, 31;
	shr.u32 	%r46, %r45, 28;
	add.s32 	%r47, %r44, %r46;
	shr.s32 	%r48, %r47, 4;
	ld.param.s32 	%rd2, [matmul_kernel_param_3];
$L__tmp2:
	.loc	2 44 22
	add.s32 	%r50, %r32, 15;
	ld.param.u32 	%r51, [matmul_kernel_param_4];
	.loc	2 44 28
	shr.s32 	%r52, %r50, 31;
	shr.u32 	%r53, %r52, 28;
	add.s32 	%r54, %r50, %r53;
	shr.s32 	%r55, %r54, 4;
$L__tmp3:
	.loc	1 61 38
	shl.b32 	%r56, %r48, 3;
	.loc	1 62 22
	div.s32 	%r58, %r35, %r56;
	.loc	1 63 29
	shl.b32 	%r59, %r58, 3;
	.loc	1 64 35
	sub.s32 	%r60, %r55, %r59;
	.loc	1 64 48
	min.s32 	%r61, %r60, 8;
	.loc	1 65 33
	rem.s32 	%r62, %r35, %r61;
	.loc	1 65 27
	add.s32 	%r63, %r59, %r62;
	mul.lo.s32 	%r64, %r58, %r56;
	sub.s32 	%r65, %r35, %r64;
	.loc	1 66 40
	div.s32 	%r66, %r65, %r61;
	.loc	1 67 29
	shl.b32 	%r67, %r63, 4;
	.loc	1 68 29
	shl.b32 	%r68, %r66, 4;
	.loc	1 76 8
	cvt.s64.s32 	%rd27, %r32;
	cvt.s64.s32 	%rd1, %r34;
	cvt.s64.s32 	%rd3, %r67;
	.loc	1 84 8
	cvt.s64.s32 	%rd28, %r33;
	cvt.s64.s32 	%rd4, %r51;
	cvt.s64.s32 	%rd5, %r68;
	.loc	1 88 43
	mov.u32 	%r1, %tid.x;
	bfe.u32 	%r2, %r1, 5, 2;
	bfe.u32 	%r3, %r1, 3, 2;
	shl.b32 	%r4, %r2, 2;
	or.b32  	%r5, %r4, %r3;
	and.b32  	%r6, %r1, 3;
	shl.b32 	%r69, %r6, 2;
	and.b32  	%r7, %r1, 7;
	cvt.u64.u32 	%rd29, %r5;
	cvt.u64.u32 	%rd6, %r69;
	.loc	1 94 20
	or.b64  	%rd30, %rd3, %rd29;
	mul.lo.s64 	%rd31, %rd30, %rd2;
	shl.b32 	%r70, %r7, 2;
	bfe.u32 	%r8, %r1, 2, 3;
	shl.b32 	%r9, %r2, 3;
	or.b32  	%r71, %r9, %r8;
	cvt.u64.u32 	%rd7, %r70;
	cvt.u64.u32 	%rd32, %r71;
	setp.gt.s64 	%p7, %rd30, -1;
	setp.lt.s64 	%p8, %rd30, %rd27;
	and.pred  	%p1, %p7, %p8;
	.loc	1 95 20
	or.b64  	%rd33, %rd5, %rd6;
	setp.gt.s64 	%p9, %rd33, -1;
	setp.lt.s64 	%p10, %rd33, %rd28;
	and.pred  	%p2, %p9, %p10;
	.loc	1 93 25
	setp.gt.s32 	%p11, %r34, 0;
	.loc	1 94 20
	add.s64 	%rd34, %rd20, %rd31;
	add.s64 	%rd23, %rd34, %rd7;
	setp.lt.s32 	%p12, %r70, %r34;
	bfe.u32 	%r10, %r1, 5, 1;
	shl.b32 	%r72, %r5, 5;
	bfe.u32 	%r11, %r1, 2, 1;
	xor.b32  	%r73, %r11, %r10;
	shl.b32 	%r74, %r73, 4;
	shl.b32 	%r75, %r1, 2;
	and.b32  	%r76, %r75, 12;
	or.b32  	%r77, %r74, %r76;
	or.b32  	%r78, %r72, %r77;
	mov.u32 	%r79, global_smem;
	add.s32 	%r36, %r79, %r78;
	selp.b32 	%r80, 4, 0, %p1;
	selp.b32 	%r81, %r80, 0, %p12;
	selp.b32 	%r37, %r81, 0, %p11;
	mov.pred 	%p6, -1;
	// begin inline asm
	@%p6 cp.async.ca.shared.global [ %r36 + 0 ], [ %rd23 + 0 ], 0x4, %r37;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 95 20
	mul.lo.s64 	%rd35, %rd4, %rd32;
	add.s64 	%rd36, %rd21, %rd35;
	add.s64 	%rd24, %rd36, %rd33;
	setp.lt.s32 	%p13, %r71, %r34;
	shl.b32 	%r82, %r71, 4;
	bfe.u32 	%r83, %r1, 1, 1;
	xor.b32  	%r84, %r83, %r10;
	shl.b32 	%r13, %r84, 3;
	and.b32  	%r85, %r75, 4;
	or.b32  	%r86, %r13, %r85;
	or.b32  	%r87, %r82, %r86;
	add.s32 	%r88, %r79, %r87;
	add.s32 	%r14, %r88, 1024;
	selp.b32 	%r89, 4, 0, %p2;
	selp.b32 	%r90, %r89, 0, %p13;
	selp.b32 	%r39, %r90, 0, %p11;
	// begin inline asm
	@%p6 cp.async.ca.shared.global [ %r14 + 0 ], [ %rd24 + 0 ], 0x4, %r39;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	setp.gt.s32 	%p14, %r34, 32;
	.loc	1 94 20
	or.b64  	%rd37, %rd7, 32;
	add.s64 	%rd25, %rd23, 32;
	setp.lt.s64 	%p15, %rd37, %rd1;
	bar.sync 	0;
	add.s32 	%r40, %r36, 512;
	selp.b32 	%r91, %r80, 0, %p15;
	selp.b32 	%r41, %r91, 0, %p14;
	// begin inline asm
	@%p6 cp.async.ca.shared.global [ %r40 + 0 ], [ %rd25 + 0 ], 0x4, %r41;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 95 20
	or.b64  	%rd38, %rd32, 32;
	mul.wide.s32 	%rd8, %r51, 32;
	add.s64 	%rd26, %rd24, %rd8;
	setp.lt.s64 	%p16, %rd38, %rd1;
	add.s32 	%r42, %r88, 1536;
	selp.b32 	%r92, %r89, 0, %p16;
	selp.b32 	%r43, %r92, 0, %p14;
	// begin inline asm
	@%p6 cp.async.ca.shared.global [ %r42 + 0 ], [ %rd26 + 0 ], 0x4, %r43;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 94 20
	// begin inline asm
	cp.async.wait_group 0x2;
	// end inline asm
	bar.sync 	0;
	bfe.u32 	%r15, %r1, 4, 1;
	.loc	1 93 25
	@%p11 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	.loc	1 0 25
	add.s32 	%r16, %r34, -64;
	and.b32  	%r101, %r1, 15;
	shl.b32 	%r102, %r6, 1;
	shl.b32 	%r103, %r10, 3;
	or.b32  	%r104, %r103, %r102;
	mad.lo.s32 	%r105, %r8, 18, %r104;
	shl.b32 	%r106, %r105, 2;
	add.s32 	%r107, %r79, 2048;
	add.s32 	%r17, %r107, %r106;
	shl.b32 	%r108, %r2, 1;
	or.b32  	%r109, %r108, %r15;
	mad.lo.s32 	%r110, %r109, 18, %r101;
	cvt.u64.u32 	%rd59, %r110;
	shl.b32 	%r111, %r110, 2;
	add.s32 	%r18, %r107, %r111;
	xor.b32  	%r112, %r15, %r11;
	shl.b32 	%r113, %r101, 5;
	shl.b32 	%r114, %r112, 4;
	or.b32  	%r19, %r114, %r113;
	or.b32  	%r20, %r13, %r8;
	shl.b32 	%r21, %r6, 6;
	or.b32  	%r22, %r20, %r21;
	.loc	1 93 25
	add.s32 	%r115, %r3, %r4;
	cvt.u64.u32 	%rd40, %r115;
	add.s64 	%rd41, %rd3, %rd40;
	mul.lo.s64 	%rd42, %rd41, %rd2;
	add.s64 	%rd43, %rd42, %rd7;
	add.s64 	%rd44, %rd43, %rd20;
	add.s64 	%rd11, %rd44, 64;
	add.s32 	%r116, %r8, %r9;
	cvt.u64.u32 	%rd45, %r116;
	or.b64  	%rd12, %rd45, 64;
	mul.lo.s64 	%rd46, %rd12, %rd4;
	add.s64 	%rd47, %rd46, %rd6;
	add.s64 	%rd48, %rd47, %rd5;
	add.s64 	%rd57, %rd21, %rd48;
	add.s32 	%r179, %r79, 1024;
	or.b64  	%rd14, %rd7, 64;
	mov.f32 	%f13, 0f00000000;
	mov.b32 	%r182, 1;
	mov.b32 	%r181, 0;
	mov.u64 	%rd58, 0;
	mov.u32 	%r180, %r79;
	mov.f32 	%f21, %f13;
	mov.f32 	%f22, %f13;
$L__BB0_3:
	.loc	1 0 25
	cvt.u32.u64 	%r132, %rd58;
	.loc	1 93 25
	setp.lt.s32 	%p19, %r132, %r16;
	.loc	1 94 20
	add.s32 	%r121, %r180, %r19;
	// begin inline asm
	ldmatrix.sync.aligned.m8n8.x4.shared.b16 { %r117, %r118, %r119, %r120 }, [ %r121 + 0 ];
	// end inline asm
	.loc	1 95 20
	add.s32 	%r133, %r179, %r22;
	add.s32 	%r134, %r20, %r21;
	add.s32 	%r135, %r179, %r134;
	ld.shared.u8 	%r136, [%r133];
	ld.shared.u8 	%r137, [%r135+16];
	bfi.b32 	%r138, %r137, %r136, 8, 8;
	ld.shared.u8 	%r139, [%r135+32];
	bfi.b32 	%r140, %r139, %r138, 16, 8;
	ld.shared.u8 	%r141, [%r135+48];
	bfi.b32 	%r126, %r141, %r140, 24, 8;
	ld.shared.u8 	%r142, [%r133+256];
	ld.shared.u8 	%r143, [%r135+272];
	bfi.b32 	%r144, %r143, %r142, 8, 8;
	ld.shared.u8 	%r145, [%r135+288];
	bfi.b32 	%r146, %r145, %r144, 16, 8;
	ld.shared.u8 	%r147, [%r135+304];
	bfi.b32 	%r127, %r147, %r146, 24, 8;
	.loc	1 96 23
	mov.f32 	%f9, %f13;
	mov.f32 	%f10, %f13;
	mov.f32 	%f11, %f13;
	mov.f32 	%f12, %f13;
	// begin inline asm
	mma.sync.aligned.m16n8k32.row.col.f32.e4m3.e4m3.f32 { %f9, %f10, %f11, %f12 }, { %r117, %r118, %r119, %r120 }, { %r126, %r127 }, { %f9, %f10, %f11, %f12 };
	// end inline asm
	st.shared.v2.f32 	[%r17], {%f9, %f10};
	st.shared.v2.f32 	[%r17+576], {%f11, %f12};
	bar.sync 	0;
	ld.shared.f32 	%f17, [%r18];
	ld.shared.f32 	%f18, [%r18+576];
	.loc	1 96 13
	add.f32 	%f21, %f21, %f17;
	add.f32 	%f22, %f22, %f18;
	.loc	1 93 25
	add.s32 	%r148, %r182, 1;
	setp.lt.s32 	%p20, %r148, 2;
	selp.b32 	%r182, %r148, 0, %p20;
	.loc	1 94 20
	add.s64 	%rd51, %rd14, %rd58;
	add.s64 	%rd49, %rd11, %rd58;
	setp.lt.u64 	%p21, %rd51, %rd1;
	shl.b32 	%r149, %r182, 9;
	bar.sync 	0;
	add.s32 	%r128, %r36, %r149;
	selp.b32 	%r150, 4, 0, %p21;
	selp.b32 	%r151, %r150, 0, %p1;
	selp.b32 	%r129, %r151, 0, %p19;
	// begin inline asm
	@%p6 cp.async.ca.shared.global [ %r128 + 0 ], [ %rd49 + 0 ], 0x4, %r129;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 95 20
	add.s64 	%rd52, %rd12, %rd58;
	setp.lt.u64 	%p22, %rd52, %rd1;
	add.s32 	%r130, %r14, %r149;
	selp.b32 	%r152, 4, 0, %p22;
	selp.b32 	%r153, %r152, 0, %p2;
	selp.b32 	%r131, %r153, 0, %p19;
	// begin inline asm
	@%p6 cp.async.ca.shared.global [ %r130 + 0 ], [ %rd57 + 0 ], 0x4, %r131;
	// end inline asm
	// begin inline asm
	cp.async.commit_group ;
	// end inline asm
	.loc	1 93 25
	add.s32 	%r154, %r181, 1;
	setp.lt.s32 	%p23, %r154, 2;
	selp.b32 	%r181, %r154, 0, %p23;
	.loc	1 94 20
	shl.b32 	%r155, %r181, 9;
	add.s32 	%r180, %r79, %r155;
	// begin inline asm
	cp.async.wait_group 0x2;
	// end inline asm
	bar.sync 	0;
	.loc	1 95 20
	add.s32 	%r179, %r180, 1024;
	.loc	1 93 25
	add.s64 	%rd58, %rd58, 32;
	add.s64 	%rd57, %rd57, %rd8;
	cvt.u32.u64 	%r157, %rd58;
	setp.lt.s32 	%p24, %r157, %r34;
	@%p24 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_4;
$L__BB0_1:
	.loc	1 103 21
	and.b32  	%r93, %r1, 15;
	shl.b32 	%r94, %r2, 1;
	or.b32  	%r95, %r94, %r15;
	mad.lo.s32 	%r96, %r95, 18, %r93;
	cvt.u64.u32 	%rd59, %r96;
	mov.f32 	%f21, 0f00000000;
	mov.f32 	%f22, %f21;
$L__BB0_4:
	.loc	1 0 21
	cvt.u32.u64 	%r166, %rd5;
	cvt.u32.u64 	%r167, %rd3;
	.loc	1 88 30
	or.b32  	%r168, %r167, %r5;
	.loc	1 91 21
	setp.lt.s32 	%p26, %r168, %r32;
	.loc	1 88 43
	shl.b32 	%r169, %r7, 1;
	.loc	1 89 30
	or.b32  	%r170, %r166, %r169;
	.loc	1 91 45
	setp.lt.s32 	%p27, %r170, %r33;
	.loc	1 91 35
	and.pred  	%p25, %p26, %p27;
	.loc	1 90 39
	mul.lo.s32 	%r171, %r168, %r31;
	.loc	1 90 21
	mul.wide.s32 	%rd54, %r171, 4;
	add.s64 	%rd55, %rd22, %rd54;
	.loc	1 90 51
	mul.wide.s32 	%rd56, %r170, 4;
	add.s64 	%rd53, %rd55, %rd56;
	.loc	1 93 25
	// begin inline asm
	cp.async.wait_group 0x0;
	// end inline asm
	bar.sync 	0;
	.loc	1 100 12
	mov.b32 	%r159, %f21;
	mov.b32 	%r160, 1073741824;
	// begin inline asm
	div.full.f32 %r158, %r159, %r160;
	// end inline asm
	mov.b32 	%r162, %f22;
	// begin inline asm
	div.full.f32 %r161, %r162, %r160;
	// end inline asm
	cvt.u32.u64 	%r172, %rd59;
	.loc	1 103 21
	shl.b32 	%r173, %r172, 2;
	add.s32 	%r175, %r79, %r173;
	st.shared.u32 	[%r175], %r158;
	st.shared.u32 	[%r175+576], %r161;
	bar.sync 	0;
	mad.lo.s32 	%r176, %r5, 18, %r169;
	shl.b32 	%r177, %r176, 2;
	add.s32 	%r178, %r79, %r177;
	ld.shared.v2.u32 	{%r164, %r165}, [%r178];
	// begin inline asm
	@%p25 st.global.v2.b32 [ %rd53 + 0 ], { %r164, %r165 };
	// end inline asm
	.loc	1 103 4
	ret;
$L__tmp4:
$L__func_end0:

}
	.file	1 "/home/csullivan/scratch/triton/python/test/unit/hopper/test_gemm_f8.py"
	.file	2 "/home/csullivan/scratch/triton/python/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 195
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 116
.b8 101
.b8 115
.b8 116
.b8 95
.b8 103
.b8 101
.b8 109
.b8 109
.b8 95
.b8 102
.b8 56
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 99
.b8 115
.b8 117
.b8 108
.b8 108
.b8 105
.b8 118
.b8 97
.b8 110
.b8 47
.b8 115
.b8 99
.b8 114
.b8 97
.b8 116
.b8 99
.b8 104
.b8 47
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 47
.b8 112
.b8 121
.b8 116
.b8 104
.b8 111
.b8 110
.b8 47
.b8 116
.b8 101
.b8 115
.b8 116
.b8 47
.b8 117
.b8 110
.b8 105
.b8 116
.b8 47
.b8 104
.b8 111
.b8 112
.b8 112
.b8 101
.b8 114
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 109
.b8 97
.b8 116
.b8 109
.b8 117
.b8 108
.b8 95
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 112
.b8 4
.b32 112
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 59
.b8 27
.b8 4
.b32 112
.b64 $L__tmp2
.b64 $L__tmp3
.b8 1
.b8 60
.b8 27
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}

